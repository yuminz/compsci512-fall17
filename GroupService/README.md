## Overview
Our GroupService is managed by a sequence of GroupServers, each running on its own actor. The management of groups is partitioned among each of the GroupServers, just like the storage of key, value pairs is partitioned across the KVStore servers. GroupServers serve two roles for this application: they manage a partition of groups and they are members of groups. When an actor wants to join a given group, it must send a JoinGroup message to the manager of that group (another GroupServer). The managing server will add the actor to the group’s membership set and push the update to the KVStore. Once the update is pushed to the KVStore, the manager sends a JoinAck to the actor, at which point the actor is considered to be part of the group. We decided to create the notion of a group manager to avoid concurrency races when multiple actors try to join the same group at the same time. Without a group manager, it is possible that one of the new members will be lost from the group since reading a value from the KVStore and then writing a new value to the KVStore is not an atomic operation. Having all group updates go through a manager serializes these updates, which ensures that no actor’s join/leave request will be lost because of the lack of atomic operations with the key value store.

Similarly, when an actor wants to leave a group, it will send a LeaveGroup message to the appropriate group manager. The manager will remove the actor’s id from the membership set for that group and push the change to the KVStore. The manager will then send a LeaveAck to the actor at which point it is said that the actor has left the group. A group manager will cache the membership sets of the groups that it manages. This speeds up the process of group membership modification since the manager can simply add to/ remove from its local copy and push the changes to the KVStore. The manager does not first have to read the current membership list from the KVStore. This caching is possible because all updates for a given group are only performed by the group manager, which means that the manager’s local copy will always be up to date.

When an actor wants to multicast a message to a group of which it is a member, it needs to read the current membership list from the KVStore. It does this by performing a direct read from the KVClient, which fetches the most up to date set from the KVStore and no caching is done. Once the actor has the group membership list, it iterates through each member and sends it the message. Since sending a multicast message only reads the value from the KVStore and doesn’t modify it, it is safe for any actor to read the membership list, not just the manager. However, only the manager can update the group membership list, which is why all join/remove requests must go through the manager. Having each member handle its multicast messages was a design decision to help prevent the group managers from being a bottleneck, and instead spreading the load to the KVStores. 

We also modified the Stats object to collect relevant statistics for our application. We keep track of the number of group joins, leaves, and send multicast message operations performed across all nodes. We also track the number of multicast messages sent within each group.



### Do actors ever receive messages originating from a given actor out of order? What if the messages are forwarded through an intermediary?
Akka guarantees that if you have actors A and B, all the messages A sends to B will be received by B in the same order they were sent. So the answer to the first question is no. If A is first sending its messages to an intermediary, C, who then forwards them to B, B will still receive all of A’s messages in order, provided that all C does is forward the message on. Specifically, C will receive all of A’s messages in order, meaning that it forwards each of A’s messages to B in order. B is then guaranteed to receive all of C’s messages in order, meaning it will also receive all of A’s messages in order from C.
	
### What if two actors multicast to the same group? Does each member receive the messages in the same order?
No, it is not guaranteed that each member will receive the messages in the same order as they were sent. If we want to make sure each member is receiving the multicast messages in the same order, we would make those multicast messages sent to the group manager server who would then distribute the message to the rest of the group members in order. However, such implementation would likely create an unnecessary bottleneck at the group manager server.

### Do actors ever receive messages for groups "late", after having left the group?
In our application, we define the moment an actor leaves a group to be when that actor receives the LeaveAck message. This confirms that the actor has been removed from the KVStore’s member list for that group. However, it is possible that another actor loaded the membership list of the group from the KVStore before this member was removed from the list. If this other actor sent out a multicast message that didn’t arrive at the actor until after the LeaveAck had arrived, the receiving actor would seem to get a message from a group of which it is no longer a member.

### How does your choice of weights and parameters affect the amount of message traffic?
There are two sources of randomness in the application: the choice of which operation to perform (join/leave/multicast) and the choice of which group to interact with. We encapsulate the randomness in the Picker trait with two different implementations which provide distinct distributions of their random choices. In both implementations, we set parameters so that an actor tries to join a group 35% of the time, leave a group 10% of the time, and send a multicast message to group 55% of the time. We chose this distribution of commands to model what we see as the expected workload for a messaging application. Additionally, the favoring of sending multicast messages allows the application to scale better because this operation does not require the intervention of the group manager. If we instead had more of the commands become join/leave operations, we would likely see the group manager become a bottleneck earlier as these operations require the help of the group manager.

When an actor is trying to join/leave/send multicast to a group, we have two ways that the actor can decide which group to interact with. The first method is the UniformPicker which randomly picks the group with uniform distribution. The other method uses the SkewedPicker, which favors the first group in the list of available groups. More specifically, when choosing which group to join with the SkewedPicker, it is most likely that the lowest number group the actor isn’t yet in will be joined. When looking at scalability, the UniformPicker scales up better since it makes the join and leave operations uniformly distributed across all groups, meaning that each group manager will get an even share of the group management load. With the skewed picker, however, since the groups with lower numbers are more likely to have more group management work, the managers of these groups are likely to become a bottleneck earlier than what happens with the UniformPicker.

We have tested our application with varying operations per node, varying burst size, and the relative number of groups per server. With each of these sets of parameters, we obtained the expected distribution of operations as discussed in the two previous paragraphs. For example, to show the Uniform distribution, you can set the opsPerNode = 100,000, burstSize = 1,000, numNodes = 10, and groupsPerNode = 10. And Set the picker to UniformPicker in the KVAppService, you will see that the number of multicast messages sent within each group is relatively even, because of the uniform distribution. Furthermore, the total number of multicast messages sent was at the expected 55% of all operations performed. However, from the stats, we can see messagesFromMaster=1000000 groupsjoined=45957 goupsLeft=45418. Here we notice that groups joined is lower than we expect. We expect group joined to be 1,000,000 * 35% = 350,000. Since we have 10 servers, we are expecting each server to join 35,000 groups. Even taking the leave group operations into consideration, the number of join group operations is still much higher than the number of groups, which is just 100. Therefore, we expect some of the join group operations to be ignored, simply because the server is already in every possible group. 

Another interesting test case uses the same parameters as before, but set the picker to SkewedPicker in KVAppService. You see that the first few groups have many more multicast messages than the others, because of the skewed distribution for group choice.  We see the distribution of join/leave/multicast operations performed is similar to that of the last experiment.

### How can you be sure your code is working, i.e., the right actors are receiving the right messages?
The stats we collect show that the distribution of Join/Leave/SendMessage operations corresponds with the distribution we intended. Also, the distribution of messages sent within groups matches the expected distributions when we use the Uniform Picker and the Skewed Picker. In addition, we wrote three unit tests to help verify the functionality of our code (please refer to GroupService/src/test/scala/GroupServiceTest.scala for detailed implementation). The first unit test verifies that when a group manager adds/removes an actor to/from a group, it correctly sends an acknowledgement message to the actor. The JoinAck and LeaveAck messages are crucial to our application as an actor does not consider itself part of a group until it receives the JoinAck. The second unit test verifies that the management of groups is partitioned across the GroupServers. The test actor will try to join 8 different groups, and we verify that each of the MockGroupServers receives the same number of JoinGroup messages. The third test verifies that actors can concurrently try to join the same group by sending join requests to the group’s manager.
